{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import to_rgba\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.rc('font', size=14)\n",
    "matplotlib.rc('axes', titlesize=14)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary function to read specific types of files\n",
    "def read_only_full(file):\n",
    "    res = np.zeros((300,5))\n",
    "    with open(file, 'r') as r:\n",
    "        num_line = 0\n",
    "        for line in r:\n",
    "            line = line[line.find(\"[\")+1:line.find(\"]\")]\n",
    "            line_split=line.split(\" \")\n",
    "            for ix, x in enumerate(line_split):\n",
    "                res[num_line, ix] = float(x)\n",
    "            num_line += 1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "path = \"results/\"\n",
    "#metrics = [\"Accuracy\", \"Macro F1\", \"Micro F1\", \"Log loss\", \"Brier score\"]\n",
    "metrics = [\"Accuracy\", \"F1\", \"F1\", \"Log loss\", \"Brier score\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fig A\n",
    "## Setting\n",
    "To control:\n",
    "- generative model: simple or complex\n",
    "- sampling or deterministic\n",
    "  - No. of dataset already computed\n",
    "- Real model result's as maximum or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "css = np.array([2,3,4,5])\n",
    "wsp = np.array([0.5,1,2,5])\n",
    "n_models = 30\n",
    "n_datasets_gen = 30\n",
    "n_datasets = 30\n",
    "\n",
    "#kdb=1; gm=\"sg\" #simple gen model\n",
    "kdb=4; gm=\"cg\" #complex gen model\n",
    "n_rep = 1; directory = \"figA_determ/\"\n",
    "#n_rep = 5; directory = \"figA_sampling/\"\n",
    "prev_name=\"Diff. \"; real_tsdata = -1 # any from 0 to 4 ; -1 means no max.\n",
    "#prev_name=\"Rel. diff. \"; real_tsdata = 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect results with only fully labeled data - synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mResultsFull = np.loadtxt(path+'res_exp_figA_'+gm+'_fully_labeled.resout', delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect results with real model - synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mResultsReal = np.loadtxt(path+'res_exp_figA_'+gm+'_real_model.resout', delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect data for `honest` scenario - synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z1_complete = []\n",
    "s = 0\n",
    "\n",
    "for m in np.arange(n_models):\n",
    "    for d in np.arange(n_datasets):\n",
    "        id_mrf = m*n_datasets_gen+d\n",
    "        id_mrr = (m*n_datasets_gen+d)*5+real_tsdata\n",
    "        for c in css:\n",
    "            orig_data = np.loadtxt(path+directory+'res_exp_figA_m_' + str(m) + \n",
    "                                                                '_d_' + str(d) + \n",
    "                                                                '_css_' + str(c) + \n",
    "                                                                '_k_' + str(kdb) + \n",
    "                                                                '_s_' + str(s) + \n",
    "                                                                 '.csv', delimiter=\",\")\n",
    "            for v in np.arange(len(wsp)):\n",
    "                vals = orig_data[np.arange(v, orig_data.shape[0], len(wsp)),:]\n",
    "                vals = np.mean(vals, axis=0) - mResultsFull[id_mrf] # mean over repetitions and remove real values\n",
    "                if real_tsdata >= 0: vals /= (mResultsReal[id_mrr] - mResultsFull[id_mrf])\n",
    "                Z1_complete.append( vals ) \n",
    "Z1_complete = np.array(Z1_complete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect data for `misleading` scenario - synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z2_complete = []\n",
    "s = 1\n",
    "\n",
    "for m in np.arange(n_models):\n",
    "    for d in np.arange(n_datasets):\n",
    "        id_mrf = m*n_datasets_gen+d\n",
    "        id_mrr = (m*n_datasets_gen+d)*5+real_tsdata\n",
    "        for c in css:\n",
    "            orig_data = np.loadtxt(path+directory+'res_exp_figA_m_' + str(m) + \n",
    "                                                                '_d_' + str(d) + \n",
    "                                                                '_css_' + str(c) + \n",
    "                                                                '_k_' + str(kdb) + \n",
    "                                                                '_s_' + str(s) + \n",
    "                                                                 '.csv', delimiter=\",\")\n",
    "            for v in np.arange(len(wsp)):\n",
    "                vals = orig_data[np.arange(v, orig_data.shape[0], len(wsp)),:]\n",
    "                vals = np.mean(vals, axis=0) - mResultsFull[id_mrf] # mean over repetitions and remove real values\n",
    "                if real_tsdata >= 0: vals /= (mResultsReal[id_mrr] - mResultsFull[id_mrf])\n",
    "                Z2_complete.append( vals ) \n",
    "Z2_complete = np.array(Z2_complete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect results for real datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_names=[\"birdac\", \"lost\", \"MSRCv2\"]\n",
    "num_fss = np.round((1+np.arange(10))*(100.0/3)).astype(int)\n",
    "nrep=30\n",
    "\n",
    "kdb=1; gm=\"cg\" #simple learnt, simple gen models\n",
    "nls=[6,6,6]\n",
    "fss=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assuming complete information (real model)\n",
    "Real model is simulated (learned with real labels and the whole dataset), as in real datasets it is not available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrealResultsComplete=[]\n",
    "\n",
    "for d in np.arange(3):\n",
    "    orig_data = np.loadtxt(path+'realdat_realmodel/res_exp_realdat_real_model_gen_' + gm + \n",
    "                                                        '_d_' + str(d) + \n",
    "                                                        '_nl_' + str(nls[d]) + \n",
    "                                                         '.csv', delimiter=\",\")\n",
    "    mrealResultsComplete.append(orig_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning only with really labeled subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrealResultsOFull=[]\n",
    "\n",
    "for d in np.arange(3):\n",
    "    orig_data = read_only_full(path+'realdat_onlyfull/res_exp_realdat_fully_labeled_gen_' + gm + \n",
    "                                                            '_kdb_' + str(kdb) + \n",
    "                                                        '_d_' + str(d) + \n",
    "                                                        '_nl_' + str(nls[d]) + \n",
    "                                                         '.resout')\n",
    "    orig_data = orig_data[np.arange(fss, orig_data.shape[0], len(num_fss)),:]\n",
    "    mrealResultsOFull.append(orig_data)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning in weakly labeled scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Zr_complete_mean = []\n",
    "Zr_complete_std = []\n",
    "\n",
    "for d in np.arange(3):\n",
    "    id_mrf = m*n_datasets_gen+d\n",
    "    id_mrr = (m*n_datasets_gen+d)*5+real_tsdata\n",
    "    orig_data = np.zeros((4,5,nrep))\n",
    "    for r in np.arange(nrep):\n",
    "        act_data = np.loadtxt(path+'realdat_weak/res_exp_realdat_weak_gen_' + gm + \n",
    "                                                            '_kdb_' + str(kdb) + \n",
    "                                                            '_d_' + str(d) + \n",
    "                                                            '_nl_' + str(nls[d]) + \n",
    "                                                            '_rep_' + str(r) + \n",
    "                                                            '_fss_' + str(fss) + \n",
    "                                                            '.csv', delimiter=\",\")\n",
    "        if np.all(act_data == -np.inf):\n",
    "            orig_data[:,:]=np.nan\n",
    "            continue\n",
    "        id_rr = (r*10+fss)*5+real_tsdata\n",
    "        act_data -= mrealResultsOFull[d][r,:]\n",
    "        if real_tsdata >= 0: act_data /= (mrealResultsComplete[d][id_rr,:] - mrealResultsOFull[d][r,:])\n",
    "        orig_data[:,:,r] = act_data\n",
    "        \n",
    "    Zr_complete_mean.append( orig_data.mean(axis=2) ) \n",
    "    Zr_complete_std.append( orig_data.std(axis=2) ) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "## 0:Acc, 1:Macro F1, 2:Micro F1, 3:log loss, 4:Brier score\n",
    "metric_ind = 1\n",
    "transparencia = 0.6\n",
    "\n",
    "\n",
    "Z1 = Z1_complete[:,metric_ind]\n",
    "Z1.shape = (n_models*n_datasets,len(css)*len(wsp))\n",
    "Z1 = Z1.mean(axis=0)\n",
    "Z1.shape = (len(css),len(wsp))\n",
    "Z1 = np.transpose(Z1)\n",
    "Z1p = (Z1 - np.min(Z1))/(np.max(Z1) - np.min(Z1))\n",
    "\n",
    "Z2 = Z2_complete[:,metric_ind]\n",
    "Z2.shape = (n_models*n_datasets,len(css)*len(wsp))\n",
    "Z2 = Z2.mean(axis=0)\n",
    "Z2.shape = (len(css),len(wsp))\n",
    "Z2 = np.transpose(Z2)\n",
    "Z2p = (Z2 - np.min(Z2))/(np.max(Z2) - np.min(Z2))\n",
    "\n",
    "\n",
    "x_labels = wsp\n",
    "x = np.arange(len(x_labels))\n",
    "\n",
    "\n",
    "### print figure\n",
    "fig = plt.figure(figsize=(7,5))\n",
    "plt_synth =[]\n",
    "aux, = plt.plot(x,Z1[:,0], \":\", label=\"Honest, |S|=2\")\n",
    "plt_synth.append(aux)\n",
    "aux, = plt.plot(x,Z1[:,1], \":\", label=\"Honest, |S|=3\")\n",
    "plt_synth.append(aux)\n",
    "\n",
    "aux, = plt.plot(x,Z2[:,0], \"-.\", label=\"Misleading, |S|=2\")\n",
    "plt_synth.append(aux)\n",
    "aux, = plt.plot(x,Z2[:,1], \"-.\", label=\"Misleading, |S|=3\")\n",
    "plt_synth.append(aux)\n",
    "\n",
    "\n",
    "plt_real =[]\n",
    "all_colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n",
    "for d in np.arange(3):\n",
    "    aux=plt.errorbar(x+0.02*d, Zr_complete_mean[d][:,metric_ind], yerr=Zr_complete_std[d][:,metric_ind],\n",
    "                     c=all_colors[d+6], label=datasets_names[d])\n",
    "    plt_real.append(aux)\n",
    "\n",
    "\n",
    "plt.xlabel(\"Prop. weakly labeled ex.\")\n",
    "plt.ylabel(prev_name+metrics[metric_ind])\n",
    "\n",
    "plt.xticks(x,labels=x_labels)\n",
    "\n",
    "#legend = plt.legend()\n",
    "legend1 = plt.legend(handles=plt_real,loc=[0.46,0.73])\n",
    "plt.legend(handles=plt_synth,  loc=\"upper left\")\n",
    "plt.gca().add_artist(legend1)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FigB\n",
    "\n",
    "## Setting\n",
    "To control:\n",
    "- generative model: simple or complex\n",
    "- sampling or deterministic\n",
    "  - No. of dataset already computed\n",
    "- Real model result's as maximum or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_fss = np.arange(10)#np.array([0,1,3,6,9])\n",
    "fss = np.round((1+np.arange(10))*(100.0/3)).astype(int)\n",
    "fss = fss[ind_fss]\n",
    "n_fss = 10\n",
    "wsp = np.array([0.5,1,2,5])\n",
    "n_models = 30\n",
    "n_datasets_gen = 30\n",
    "n_datasets = 30\n",
    "\n",
    "#kdb=1; gm=\"sg\" #simple gen model\n",
    "kdb=4; gm=\"cg\" #complex gen model\n",
    "n_rep = 1; directory = \"figB_determ/\"\n",
    "#n_rep = 5; directory = \"figB_sampling/\"\n",
    "prev_name=\"Diff. \"; real_tsdata = -1 # any from 0 to 4 ; -1 means no max.3\n",
    "#prev_name=\"Rel. diff. \"; real_tsdata = 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect results with only fully labeled data - synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mResultsFull = np.loadtxt(path+'res_exp_figB_'+gm+'_fully_labeled.resout', delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect results with real model - synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mResultsReal = np.loadtxt(path+'res_exp_figB_'+gm+'_real_model.resout', delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect data for `honest` scenario - synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z1_complete = []\n",
    "s = 0\n",
    "\n",
    "for m in np.arange(n_models):\n",
    "    for d in np.arange(n_datasets):\n",
    "        for f in ind_fss:\n",
    "            orig_data = np.loadtxt(path+directory+'res_exp_figB_m_' + str(m) + \n",
    "                                                                '_d_' + str(d) + \n",
    "                                                                '_fss_' + str(f) + \n",
    "                                                                '_k_' + str(kdb) + \n",
    "                                                                '_s_' + str(s) + \n",
    "                                                                 '.csv', delimiter=\",\")\n",
    "            id_mrf = (m*n_datasets_gen+d)*n_fss+f\n",
    "            id_mrr = ((m*n_datasets_gen+d)*n_fss+f)*5+real_tsdata\n",
    "            for v in np.arange(len(wsp)):\n",
    "                vals = orig_data[np.arange(v, orig_data.shape[0], len(wsp)),:]\n",
    "                vals = np.mean(vals, axis=0) - mResultsFull[id_mrf] # mean over repetitions and remove real values\n",
    "                if real_tsdata >= 0: vals /= (mResultsReal[id_mrr] - mResultsFull[id_mrf])\n",
    "                Z1_complete.append( vals ) \n",
    "Z1_complete = np.array(Z1_complete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect results for real datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_names=[\"birdac\", \"lost\", \"MSRCv2\"]\n",
    "num_fss = np.round((1+np.arange(10))*(100.0/3)).astype(int)\n",
    "nrep=30\n",
    "\n",
    "kdb=1; gm=\"cg\" #simple learnt, simple gen models\n",
    "nls=[6,6,6]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assuming complete information (real model)\n",
    "Real model is simulated (learned with real labels and the whole dataset), as in real datasets it is not available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrealResultsComplete=[]\n",
    "for d in np.arange(3):\n",
    "    orig_data = np.loadtxt(path+'realdat_realmodel/res_exp_realdat_real_model_gen_' + gm + \n",
    "                                                        '_d_' + str(d) + \n",
    "                                                        '_nl_' + str(nls[d]) + \n",
    "                                                        '.csv', delimiter=\",\")\n",
    "    mrealResultsComplete.append(orig_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning only with really labeled subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrealResultsOFull=[]\n",
    "\n",
    "for d in np.arange(3):\n",
    "    orig_data = read_only_full(path+'realdat_onlyfull/res_exp_realdat_fully_labeled_gen_' + gm + \n",
    "                                                        '_kdb_' + str(kdb) + \n",
    "                                                        '_d_' + str(d) + \n",
    "                                                        '_nl_' + str(nls[d]) + \n",
    "                                                        '.resout')\n",
    "    mrealResultsOFull.append(orig_data)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning in weakly labeled scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Zr_complete = []\n",
    "\n",
    "for d in np.arange(3):\n",
    "    Zr_d = np.empty((0,5))\n",
    "    for f in ind_fss:\n",
    "        orig_data = np.zeros((4,5))\n",
    "        for r in np.arange(nrep):\n",
    "            act_data = np.loadtxt(path+'realdat_weak/res_exp_realdat_weak_gen_' + gm + \n",
    "                                                                '_kdb_' + str(kdb) + \n",
    "                                                                '_d_' + str(d) + \n",
    "                                                                '_nl_' + str(nls[d]) + \n",
    "                                                                '_rep_' + str(r) + \n",
    "                                                                '_fss_' + str(f) + \n",
    "                                                                 '.csv', delimiter=\",\")\n",
    "            if act_data.shape[0]==5:\n",
    "                act_data=act_data[:4,:]\n",
    "            id_rr = (r*10+f)*5+real_tsdata\n",
    "            id_of = r*10+f\n",
    "            act_data -= mrealResultsOFull[d][id_of,:]\n",
    "            if real_tsdata >= 0: act_data /= (mrealResultsComplete[d][id_rr,:] - mrealResultsOFull[d][id_of,:])\n",
    "            orig_data += act_data\n",
    "        orig_data /= nrep\n",
    "        \n",
    "        Zr_d= np.append( Zr_d, orig_data, axis=0 ) \n",
    "    Zr_complete.append( np.array(Zr_d) ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "## 0:Acc, 1:Macro F1, 2:Micro F1, 3:log loss, 4:Brier score\n",
    "metric_ind = 1\n",
    "transparencia = 0.6\n",
    "dataset=1\n",
    "\n",
    "Z1 = Z1_complete[:,metric_ind]\n",
    "Z1.shape = (n_models*n_datasets,len(fss)*len(wsp))\n",
    "Z1 = Z1.mean(axis=0)\n",
    "Z1.shape = (len(fss),len(wsp))\n",
    "Z1 = np.transpose(Z1)\n",
    "Z1p = (Z1 - np.min(Z1))/(np.max(Z1) - np.min(Z1))\n",
    "\n",
    "Z2 = Zr_complete[dataset][:,metric_ind]\n",
    "Z2.shape = (len(fss),len(wsp))\n",
    "Z2 = np.transpose(Z2)\n",
    "Z2p = (Z2 - np.min(Z2))/(np.max(Z2) - np.min(Z2))\n",
    "\n",
    "\n",
    "x = np.arange(len(fss))\n",
    "x_labels = fss\n",
    "y = np.arange(len(wsp))\n",
    "y_labels = wsp\n",
    "\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "\n",
    "C1 = np.empty_like(Z1, dtype=object)\n",
    "C2 = np.empty_like(Z2, dtype=object)\n",
    "cmap1 = plt.get_cmap(\"spring\")\n",
    "cmap2 = plt.get_cmap(\"winter\")\n",
    "\n",
    "for i in range(Z1.shape[0]):\n",
    "    for j in range(Z1.shape[1]):\n",
    "        C1[i,j] = to_rgba(cmap1(Z1p[i,j]), transparencia)\n",
    "        C2[i,j] = to_rgba(cmap2(Z2p[i,j]), transparencia)\n",
    "\n",
    "\n",
    "# Create a transparent bridge region\n",
    "X_bridge = np.vstack([X[-1,:],X[-1,:]])\n",
    "Y_bridge = np.vstack([Y[-1,:],Y[-1,:]])\n",
    "Z_bridge = np.vstack([Z1[-1,:],Z2[-1,:]])\n",
    "color_bridge = np.empty_like(Z_bridge, dtype=object)\n",
    "\n",
    "color_bridge.fill((1,1,1,0)) # RGBA colour, onlt the last component matters.\n",
    "\n",
    "# Join the two surfaces flipping one of them (using also the bridge)\n",
    "X_full = np.vstack([X, X_bridge, np.flipud(X)])\n",
    "Y_full = np.vstack([Y, Y_bridge, np.flipud(Y)])\n",
    "Z_full = np.vstack([Z1, Z_bridge, np.flipud(Z2)])\n",
    "color_full = np.vstack([C1, color_bridge, np.flipud(C2)])\n",
    "\n",
    "\n",
    "### print figure\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "ax = fig.gca(projection='3d')\n",
    "\n",
    "surf_full = ax.plot_surface(X_full, Y_full, Z_full, rstride=1, cstride=1,\n",
    "                            facecolors=color_full, linewidth=0,\n",
    "                            antialiased=False, shade=False)\n",
    "\n",
    "ax.set_xlabel(\"Size of the fully labeled set\")\n",
    "ax.set_ylabel(\"Prop. weakly labeled ex.\")\n",
    "ax.set_zlabel(prev_name+metrics[metric_ind])\n",
    "\n",
    "#fig.colorbar(surf_full, shrink=0.5, aspect=25, label=\"DAV\")\n",
    "ax.set_xticks(x)\n",
    "plt.xticks(rotation=45)\n",
    "ax.set_xticklabels(x_labels)\n",
    "ax.set_yticks(y)\n",
    "ax.set_yticklabels(y_labels)\n",
    "\n",
    "ax.text(3, 3, Z1[-1,0], \"Synth. data\", zdir=\"x\",\n",
    "        color=cmap1(Z1p[0,-1]))\n",
    "\n",
    "ax.text(3, 3, Z2[-1,0], \"Real data\", zdir=\"x\",\n",
    "        color=cmap2(Z2p[0,-1]))\n",
    "\n",
    "\n",
    "# manipulate view\n",
    "ax.view_init(20, -65)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FigC-1\n",
    "\n",
    "## Setting\n",
    "To control:\n",
    "- generative model: simple or complex\n",
    "- sampling or deterministic\n",
    "  - No. of dataset already computed (different from FigA and FigC)\n",
    "- Real model result's as maximum or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "css = np.array([2,3,4,5])\n",
    "amb = np.array([0.25,0.5,0.75,1.0])\n",
    "wsp_ind = 2; wsp = np.array([0.5,1,2,5])\n",
    "n_models = 30\n",
    "n_datasets_gen = 30\n",
    "n_datasets_Z1 = 30\n",
    "n_datasets_Z2 = 30\n",
    "\n",
    "#kdb=1; gm=\"sg\" #simple gen model\n",
    "kdb=4; gm=\"cg\" #complex gen model\n",
    "n_rep = 1; ; directory_Z1 = \"figC_determ/\"; directory_Z2 = \"figC_determ/\"\n",
    "#n_rep = 5; directory_Z1 = \"figC_sampling/\"; directory_Z2 = \"figC_sampling/\"\n",
    "prev_name=\"Diff. \"; real_tsdata = -1 # any from 0 to 4 ; -1 means no max.\n",
    "#prev_name=\"Rel. diff. \"; real_tsdata = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect results with only fully labeled data - synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mResultsFull = np.loadtxt(path+'res_exp_figA_'+gm+'_fully_labeled.resout', delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect results with real model - synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mResultsReal = np.loadtxt(path+'res_exp_figA_'+gm+'_real_model.resout', delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect data for `consistent-labeling` scenario - synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z1_complete = []\n",
    "\n",
    "for m in np.arange(n_models):\n",
    "    for d in np.arange(n_datasets_Z1):\n",
    "        id_mrf = m*n_datasets_gen+d\n",
    "        id_mrr = (m*n_datasets_gen+d)*5+real_tsdata\n",
    "        for c in css:\n",
    "            for a in np.arange(len(amb))+1:\n",
    "                orig_data = np.loadtxt(path+directory_Z1+'res_exp_figC_m_' + str(m) + \n",
    "                                                                    '_d_' + str(d) + \n",
    "                                                                    '_css_' + str(c) + \n",
    "                                                                    '_k_' + str(kdb) + \n",
    "                                                                    '_s_' + str(a) + \n",
    "                                                                     '.csv', delimiter=\",\")\n",
    "                vals = orig_data[np.arange(wsp_ind, orig_data.shape[0], len(wsp)),:]\n",
    "                vals = np.mean(vals, axis=0) - mResultsFull[id_mrf] # mean over repetitions and remove real values\n",
    "                if real_tsdata >= 0: vals /= (mResultsReal[id_mrr] - mResultsFull[id_mrf])\n",
    "                Z1_complete.append( vals ) \n",
    "Z1_complete = np.array(Z1_complete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect data for `inconsistent-labeling` scenario - synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z2_complete = []\n",
    "s = 0 # honest scenario\n",
    "\n",
    "for m in np.arange(n_models):\n",
    "    for d in np.arange(n_datasets_Z2):\n",
    "        id_mrf = m*n_datasets_gen+d\n",
    "        id_mrr = (m*n_datasets_gen+d)*5+real_tsdata\n",
    "        for c in css:\n",
    "            orig_data = np.loadtxt(path+directory_Z2+'res_exp_figA_m_' + str(m) + \n",
    "                                                                '_d_' + str(d) + \n",
    "                                                                '_css_' + str(c) + \n",
    "                                                                '_k_' + str(kdb) + \n",
    "                                                                '_s_' + str(s) + \n",
    "                                                                 '.csv', delimiter=\",\")\n",
    "            vals = orig_data[np.arange(wsp_ind, orig_data.shape[0], len(wsp)),:]\n",
    "            vals = np.mean(vals, axis=0) - mResultsFull[id_mrf] # mean over repetitions and remove real values\n",
    "            if real_tsdata >= 0: vals /= (mResultsReal[id_mrr] - mResultsFull[id_mrf])\n",
    "            Z2_complete.append( vals ) \n",
    "Z2_complete = np.array(Z2_complete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect results for real datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_names=[\"birdac\", \"lost\", \"MSRCv2\"]\n",
    "num_fss = np.round((1+np.arange(10))*(100.0/3)).astype(int)\n",
    "nrep=30\n",
    "\n",
    "kdb=1; gm=\"cg\" #simple learnt, simple gen models\n",
    "nls=[6,6,6]    \n",
    "fss=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assuming complete information (real model)\n",
    "Real model is simulated (learned with real labels and the whole dataset), as in real datasets it is not available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrealResultsComplete=[]\n",
    "\n",
    "for d in np.arange(3):\n",
    "    orig_data = np.loadtxt(path+'realdat_realmodel/res_exp_realdat_real_model_gen_' + gm + \n",
    "                                                        '_d_' + str(d) + \n",
    "                                                        '_nl_' + str(nls[d]) + \n",
    "                                                        '.csv', delimiter=\",\")\n",
    "    \n",
    "    mrealResultsComplete.append(orig_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning only with really labeled subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrealResultsOFull=[]\n",
    "\n",
    "for d in np.arange(3):\n",
    "    orig_data = read_only_full(path+'realdat_onlyfull/res_exp_realdat_fully_labeled_gen_' + gm + \n",
    "                                                        '_kdb_' + str(kdb) + \n",
    "                                                        '_d_' + str(d) + \n",
    "                                                        '_nl_' + str(nls[d]) + \n",
    "                                                        '.resout')\n",
    "    orig_data = orig_data[np.arange(fss, orig_data.shape[0], len(num_fss)),:]\n",
    "    \n",
    "    mrealResultsOFull.append(orig_data)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning in weakly labeled scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Zr_complete = []\n",
    "\n",
    "for d in np.arange(3):\n",
    "    id_mrf = m*n_datasets_gen+d\n",
    "    id_mrr = (m*n_datasets_gen+d)*5+real_tsdata\n",
    "    orig_data = np.zeros((nrep,5))/0\n",
    "    for r in np.arange(nrep):\n",
    "        act_data = np.loadtxt(path+'realdat_weak/res_exp_realdat_weak_gen_' + gm + \n",
    "                                                            '_kdb_' + str(kdb) + \n",
    "                                                            '_d_' + str(d) + \n",
    "                                                            '_nl_' + str(nls[d]) + \n",
    "                                                            '_rep_' + str(r) + \n",
    "                                                            '_fss_' + str(fss) + \n",
    "                                                             '.csv', delimiter=\",\")\n",
    "        id_rr = (r*10+fss)*5+real_tsdata\n",
    "        act_data = act_data[wsp_ind,:] - mrealResultsOFull[d][r,:]\n",
    "        if real_tsdata >= 0: act_data /= (mrealResultsComplete[d][id_rr,:] - mrealResultsOFull[d][r,:])\n",
    "        if np.abs(np.sum(act_data)) == np.inf:\n",
    "            continue;\n",
    "        orig_data[r,:] = act_data\n",
    "    \n",
    "    Zr_complete.append(orig_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_ambiguity(cand_sets, y, nlabs):\n",
    "    amb = []\n",
    "    for c in np.arange(nlabs):\n",
    "        cand_labs = np.zeros(nlabs)\n",
    "        for act_y, cs in zip(y,cand_sets):\n",
    "            if act_y == c:\n",
    "                cand_labs[cs] += 1\n",
    "\n",
    "        cand_labs /= np.sum(y == c)\n",
    "        cand_labs[c] = 0\n",
    "        amb.append(np.max(cand_labs))\n",
    "    amb = np.array(amb)\n",
    "    return amb.mean()\n",
    "\n",
    "def measure_ambiguities_used_dataset(dtst, nrl, nfi, pwi, complexity): # comp: simple/complex\n",
    "    dbfile = open('models_and_data/real_data_' + complexity + '.pkl', 'rb')\n",
    "    orig_models = pickle.load(dbfile)\n",
    "    orig_transf_data = pickle.load(dbfile)\n",
    "    orig_datasets = pickle.load(dbfile)\n",
    "    orig_transf_csets = pickle.load(dbfile)\n",
    "    dbfile.close()\n",
    "\n",
    "    iclass = orig_models[dtst][nrl-4].class_ind\n",
    "    ambs = []\n",
    "    for r in np.arange(30):\n",
    "        idx_act_dst = (nrl-4)*30*10+r*10+nfi\n",
    "        act_datasets = orig_datasets[dtst][idx_act_dst]\n",
    "        \n",
    "        if len(act_datasets) == 0:\n",
    "            ambs.append(np.nan)\n",
    "            continue\n",
    "\n",
    "        idxs_sample = act_datasets[0]\n",
    "        empty=False\n",
    "        for ind_wlp in np.arange(pwi+1):\n",
    "            if len(act_datasets[ind_wlp + 1]) == 0:\n",
    "                empty = True\n",
    "                break\n",
    "            idxs_sample = np.concatenate((idxs_sample, act_datasets[ind_wlp + 1]))\n",
    "        if empty:\n",
    "            ambs.append(np.nan)\n",
    "            continue\n",
    "\n",
    "        dataset = orig_transf_data[dtst][nrl-4][idxs_sample, :]\n",
    "        candsets = [orig_transf_csets[dtst][nrl-4][i] for i in idxs_sample]\n",
    "\n",
    "        cs_sizes = np.array([len(cs) for cs in candsets])\n",
    "        csld = np.where(cs_sizes > 1)[0]\n",
    "        really_candsets = [candsets[i] for i in csld]\n",
    "        y_csld = dataset[csld, iclass]\n",
    "\n",
    "        ambs.append( measure_ambiguity(really_candsets, y_csld, nrl) )\n",
    "    return ambs\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "## 0:Acc, 1:Macro F1, 2:Micro F1, 3:log loss, 4:Brier score\n",
    "metric_ind = 1\n",
    "transparencia = 0.6\n",
    "\n",
    "\n",
    "Z1 = Z1_complete[:,metric_ind]\n",
    "Z1.shape = (n_models*n_datasets_Z1,len(css)*len(amb))\n",
    "Z1 = Z1.mean(axis=0)\n",
    "Z1.shape = (len(css),len(amb))\n",
    "Z1 = np.transpose(Z1)\n",
    "Z1p = (Z1 - np.min(Z1))/(np.max(Z1) - np.min(Z1))\n",
    "\n",
    "Z2 = Z2_complete[:,metric_ind]\n",
    "Z2.shape = (n_models*n_datasets_Z2,len(css))\n",
    "Z2 = Z2.mean(axis=0)\n",
    "Z2 = np.repeat(Z2, len(amb)) # dimensions of Z1 and Z2 are different!!! dim(Z1) = dim(Z2)*4\n",
    "Z2.shape = (len(css),len(amb))\n",
    "Z2 = np.transpose(Z2)\n",
    "Z2p = (Z2 - np.min(Z2))/(np.max(Z2) - np.min(Z2))\n",
    "\n",
    "\n",
    "x = amb#np.arange(len(amb))\n",
    "x_labels = amb\n",
    "\n",
    "plt_synth=[]\n",
    "### print figure\n",
    "fig = plt.figure(figsize=(7,5))\n",
    "aux,=plt.plot(x,Z1[:,0], \":\", label=\"With co-ocur., |S|=2\")\n",
    "plt_synth.append(aux)\n",
    "aux,=plt.plot(x,Z1[:,1], \":\", label=\"With co-ocur., |S|=3\")\n",
    "plt_synth.append(aux)\n",
    "\n",
    "aux,=plt.plot(x,Z2[:,0], \"-.\", label=\"Without, |S|=2\")\n",
    "plt_synth.append(aux)\n",
    "aux,=plt.plot(x,Z2[:,1], \"-.\", label=\"Without, |S|=3\")\n",
    "plt_synth.append(aux)\n",
    "\n",
    "ylimits = [\n",
    "    np.min(np.concatenate((Z1[:,0],Z1[:,1],Z2[:,0],Z2[:,1]))),\n",
    "    np.max(np.concatenate((Z1[:,0],Z1[:,1],Z2[:,0],Z2[:,1])))\n",
    "]\n",
    "\n",
    "markers=['o', '^', 's']\n",
    "plt_real=[]\n",
    "all_colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n",
    "for d in np.arange(3):\n",
    "    ambs = measure_ambiguities_used_dataset(d, nls[d], fss, wsp_ind, \"simple\")\n",
    "    plt.scatter(ambs, Zr_complete[d][:,metric_ind], marker=markers[d], c=all_colors[d+6],\n",
    "                alpha=0.2, linewidths=0) #, label=datasets_names[d])\n",
    "\n",
    "    aux=plt.errorbar(np.mean(ambs),np.mean(Zr_complete[d][:,metric_ind]), \n",
    "                 xerr=np.std(ambs), yerr=np.std(Zr_complete[d][:,metric_ind]), \n",
    "                 c=all_colors[d+6], ecolor=all_colors[d+6], fmt=markers[d], capthick=2, label=datasets_names[d])\n",
    "    plt_real.append(aux)\n",
    "    \n",
    "    ylimits[0] = np.min((ylimits[0], \n",
    "                         np.mean(Zr_complete[d][:,metric_ind]) - 1.3*np.std(Zr_complete[d][:,metric_ind])))\n",
    "    ylimits[1] = np.max((ylimits[1], \n",
    "                         np.mean(Zr_complete[d][:,metric_ind]) + 1.3*np.std(Zr_complete[d][:,metric_ind])))\n",
    "    \n",
    "\n",
    "plt.xlabel(\"Prob. of co-occurrence\")\n",
    "plt.ylabel(prev_name+metrics[metric_ind])\n",
    "plt.ylim([-.12,.14])\n",
    "plt.xticks(x,labels=x_labels)\n",
    "\n",
    "#plt.legend()\n",
    "legend1 = plt.legend(handles=plt_real,loc=\"upper left\")\n",
    "plt.legend(handles=plt_synth,  loc=\"lower left\", handlelength=1)\n",
    "plt.gca().add_artist(legend1)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FigC-2\n",
    "\n",
    "## Setting\n",
    "To control:\n",
    "- generative model: simple or complex\n",
    "- sampling or deterministic\n",
    "  - No. of dataset already computed (different from FigA and FigC)\n",
    "- Real model result's as maximum or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "css_ind = 0; css = np.array([2,3,4,5])\n",
    "amb = np.array([0.25,0.5,0.75,1.0])\n",
    "wsp = np.array([0.5,1,2,5])\n",
    "n_models = 30\n",
    "n_datasets_gen = 30\n",
    "n_datasets_Z1 = 30\n",
    "n_datasets_Z2 = 30\n",
    "\n",
    "#kdb=1; gm=\"sg\" #simple gen model\n",
    "kdb=4; gm=\"cg\" #complex gen model\n",
    "n_rep = 1; directory_Z1 = \"figC_determ/\"; directory_Z2 = \"figA_determ/\"\n",
    "#n_rep = 5; directory_Z1 = \"figC_sampling/\"; directory_Z2 = \"figA_sampling/\"\n",
    "prev_name=\"Diff. \"; real_tsdata = -1 # any from 0 to 4 ; -1 means no max.\n",
    "#prev_name=\"Rel. diff. \"; real_tsdata = 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect results with only fully labeled data - synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mResultsFull = np.loadtxt(path+'res_exp_figA_'+gm+'_fully_labeled.resout', delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect results with real model - synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mResultsReal = np.loadtxt(path+'res_exp_figA_'+gm+'_real_model.resout', delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect data for `consistent-labeling` scenario - synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z1_complete = []\n",
    "inta = 0\n",
    "for m in np.arange(n_models):\n",
    "    for d in np.arange(n_datasets_Z1):\n",
    "        id_mrf = m*n_datasets_gen+d\n",
    "        id_mrr = (m*n_datasets_gen+d)*5+real_tsdata\n",
    "        for a in np.arange(len(amb))+1:\n",
    "            orig_data = np.loadtxt(path+directory_Z1+'res_exp_figC_m_' + str(m) + \n",
    "                                                                '_d_' + str(d) + \n",
    "                                                                '_css_' + str(css[css_ind]) + \n",
    "                                                                '_k_' + str(kdb) + \n",
    "                                                                '_s_' + str(a) + \n",
    "                                                                 '.csv', delimiter=\",\")\n",
    "            for v in np.arange(len(wsp)):\n",
    "                vals = orig_data[np.arange(v, orig_data.shape[0], len(wsp)),:]\n",
    "                vals = np.mean(vals, axis=0) - mResultsFull[id_mrf] # mean over repetitions and remove real values\n",
    "                if real_tsdata >= 0: vals /= (mResultsReal[id_mrr] - mResultsFull[id_mrf])\n",
    "                Z1_complete.append( vals ) \n",
    "                inta+=1\n",
    "Z1_complete = np.array(Z1_complete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect data for `inconsistent-labeling` scenario - synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z2_complete = []\n",
    "s = 0 # honest scenario\n",
    "\n",
    "for m in np.arange(n_models):\n",
    "    for d in np.arange(n_datasets_Z2):\n",
    "        id_mrf = m*n_datasets_gen+d\n",
    "        id_mrr = (m*n_datasets_gen+d)*5+real_tsdata\n",
    "        orig_data = np.loadtxt(path+directory_Z2+'res_exp_figA_m_' + str(m) + \n",
    "                                                            '_d_' + str(d) + \n",
    "                                                            '_css_' + str(css[css_ind]) + \n",
    "                                                            '_k_' + str(kdb) + \n",
    "                                                            '_s_' + str(s) + \n",
    "                                                             '.csv', delimiter=\",\")\n",
    "        for v in np.arange(len(wsp)):\n",
    "            vals = orig_data[np.arange(v, orig_data.shape[0], len(wsp)),:]\n",
    "            vals = np.mean(vals, axis=0) - mResultsFull[id_mrf] # mean over repetitions and remove real values\n",
    "            if real_tsdata >= 0: vals /= (mResultsReal[id_mrr] - mResultsFull[id_mrf])\n",
    "            Z2_complete.append( vals ) \n",
    "Z2_complete = np.array(Z2_complete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect results for real datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_names=[\"birdac\", \"lost\", \"MSRCv2\"]\n",
    "num_fss = np.round((1+np.arange(10))*(100.0/3)).astype(int)\n",
    "nrep=30\n",
    "\n",
    "kdb=1; gm=\"cg\" #simple learnt, simple gen models\n",
    "nls=[6,6,6]    \n",
    "fss=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assuming complete information (real model)\n",
    "Real model is simulated (learned with real labels and the whole dataset), as in real datasets it is not available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrealResultsComplete=[]\n",
    "\n",
    "for d in np.arange(3):\n",
    "    orig_data = np.loadtxt(path+'realdat_realmodel/res_exp_realdat_real_model_gen_' + gm + \n",
    "                                                        '_d_' + str(d) + \n",
    "                                                        '_nl_' + str(nls[d]) + \n",
    "                                                        '.csv', delimiter=\",\")\n",
    "    \n",
    "    mrealResultsComplete.append(orig_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning only with really labeled subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrealResultsOFull=[]\n",
    "\n",
    "for d in np.arange(3):\n",
    "    orig_data = read_only_full(path+'realdat_onlyfull/res_exp_realdat_fully_labeled_gen_' + gm + \n",
    "                                                        '_kdb_' + str(kdb) + \n",
    "                                                        '_d_' + str(d) + \n",
    "                                                        '_nl_' + str(nls[d]) + \n",
    "                                                        '.resout')\n",
    "    orig_data = orig_data[np.arange(fss, orig_data.shape[0], len(num_fss)),:]\n",
    "    \n",
    "    mrealResultsOFull.append(orig_data)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning in weakly labeled scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Zr_complete_mean = []\n",
    "Zr_complete_std = []\n",
    "\n",
    "for d in np.arange(3):\n",
    "    id_mrf = m*n_datasets_gen+d\n",
    "    id_mrr = (m*n_datasets_gen+d)*5+real_tsdata\n",
    "    orig_data = np.zeros((4,5,nrep))\n",
    "    for r in np.arange(nrep):\n",
    "        act_data = np.loadtxt(path+'realdat_weak/res_exp_realdat_weak_gen_' + gm + \n",
    "                                                            '_kdb_' + str(kdb) + \n",
    "                                                            '_d_' + str(d) + \n",
    "                                                            '_nl_' + str(nls[d]) + \n",
    "                                                            '_rep_' + str(r) + \n",
    "                                                            '_fss_' + str(fss) + \n",
    "                                                            '.csv', delimiter=\",\")\n",
    "        id_rr = (r*10+fss)*5+real_tsdata\n",
    "        act_data -= mrealResultsOFull[d][r,:]\n",
    "        if real_tsdata >= 0: act_data /= (mrealResultsComplete[d][id_rr,:] - mrealResultsOFull[d][r,:])\n",
    "        orig_data[:,:,r] = act_data\n",
    "    Zr_complete_mean.append( np.mean(orig_data,axis=2) ) \n",
    "    Zr_complete_std.append( np.std(orig_data,axis=2) ) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "## 0:Acc, 1:Macro F1, 2:Micro F1, 3:log loss, 4:Brier score\n",
    "metric_ind = 1\n",
    "transparencia = 0.6\n",
    "\n",
    "\n",
    "Z1 = Z1_complete[:,metric_ind]\n",
    "Z1.shape = (n_models*n_datasets_Z1,len(amb)*len(wsp))\n",
    "Z1_std = np.std(Z1,axis=0)\n",
    "Z1 = Z1.mean(axis=0)\n",
    "Z1.shape = (len(amb),len(wsp))\n",
    "Z1_std.shape = (len(amb),len(wsp))\n",
    "Z1 = np.transpose(Z1)\n",
    "Z1_std = np.transpose(Z1_std)\n",
    "Z1p = (Z1 - np.min(Z1))/(np.max(Z1) - np.min(Z1))\n",
    "\n",
    "Z2 = Z2_complete[:,metric_ind]\n",
    "Z2.shape = (n_models*n_datasets_Z2,len(wsp))\n",
    "Z2 = Z2.mean(axis=0)\n",
    "Z2 = np.tile(Z2, len(amb)) # dimensions of Z1 and Z2 are different!!! dim(Z1) = dim(Z2)*4\n",
    "Z2.shape = (len(amb),len(wsp))\n",
    "Z2 = np.transpose(Z2)\n",
    "Z2p = (Z2 - np.min(Z2))/(np.max(Z2) - np.min(Z2))\n",
    "\n",
    "\n",
    "x_labels = wsp\n",
    "x = np.arange(len(x_labels))\n",
    "\n",
    "plt_synth=[]\n",
    "### print figure\n",
    "fig = plt.figure(figsize=(7,5))\n",
    "aux,=plt.plot(x,Z1[:,1], \":\", label=\"With co-occur., s=0.5\")\n",
    "plt_synth.append(aux)\n",
    "aux,=plt.plot(x,Z1[:,2], \":\", label=\"With co-occur., s=0.75\")\n",
    "plt_synth.append(aux)\n",
    "aux,=plt.plot(x,Z2[:,0], \"-.\", label=\"Without\")\n",
    "plt_synth.append(aux)\n",
    "\n",
    "\n",
    "plt_real=[]\n",
    "all_colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n",
    "for d in np.arange(3):\n",
    "    aux=plt.errorbar(x+0.02*d, Zr_complete_mean[d][:,metric_ind], yerr=Zr_complete_std[d][:,metric_ind],\n",
    "                     c=all_colors[d+6], label=datasets_names[d])\n",
    "\n",
    "    plt_real.append(aux)\n",
    "\n",
    "\n",
    "plt.xlabel(\"Prop. weakly labeled ex.\")\n",
    "plt.ylabel(prev_name+metrics[metric_ind])\n",
    "\n",
    "plt.xticks(x,labels=x_labels)\n",
    "\n",
    "#plt.legend()\n",
    "legend1 = plt.legend(handles=plt_real,loc=\"upper left\")\n",
    "plt.legend(handles=plt_synth,  loc=\"lower right\")\n",
    "plt.gca().add_artist(legend1)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
